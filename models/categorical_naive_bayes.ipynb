{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multunomiale Naive Bayes\n",
    "\n",
    "## I) Theory\n",
    "\n",
    "Naive Bayes models are a type of classification algorithm that is fast, simple and is suitable for classification probleme because the data set is high-dimensional.\n",
    "This model will be a baseline for our Multi-Class classification problem since they are so fast and have so few configurable parameters.\n",
    "\n",
    "### 1) Equations\n",
    "\n",
    "The model is based on Bayes theorem, an equation that describes the relationship between conditional probabilities of statistical quantities.\n",
    "We want to find the probability of a Sector $S$ given certain observed features in Bayesian classification, which we can write as $P(S|rm features)$.\n",
    "We have with the Bayes theorem that :\n",
    "\n",
    "$$\n",
    "P(S~|~{\\rm features}) = \\frac{P({\\rm features}~|~S)P(S)}{P({\\rm features})}\n",
    "$$\n",
    "\n",
    "If we want to choose between two labels, let's call them $S_1$ and $S_2$, one technique to do so is to compute the ratio of their posterior probabilities:\n",
    "\n",
    "$$\n",
    "\\frac{P(S_1~|~{\\rm features})}{P(S_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~S_1)}{P({\\rm features}~|~S_2)}\\frac{P(S_1)}{P(S_2)}\n",
    "$$\n",
    "\n",
    "\n",
    "All we need now is a model that can calculate $P(rm features|S_i)$ for each label.\n",
    "A *generative model* is one that specifies specifies the hypothetical random process that generates the data.\n",
    "The fundamental component of training a Bayesian classifier is specifying this generative model for each label.\n",
    "The general version of such a training phase is a challenging operation, but we may simplify it by making some simplifying assumptions about the model's shape.\n",
    "\n",
    "In the next part of this notebook we will use Categorical Naive Bayes because it's the one that best fits our problem.\n",
    "\n",
    "### 2) Categorical Naive Bayes\n",
    "\n",
    "It is assumed that the features are generated by a basic multinomial distribution.\n",
    "Because the multinomial distribution explains the chance of detecting counts across several categories, categorical naive Bayes is best suited for features that represent counts or count rates.\n",
    "\n",
    "## Advantages and Drawbacks\n",
    "\n",
    "Advantages :\n",
    "- This algorithm works very fast and can easily predict the class of a test dataset. \n",
    "- For very high-dimensional data, when model complexity is less important\n",
    "- Naive Bayes classifier performs better than other models with less training data if the assumption of independence of features holds. \n",
    "- They provide straightforward probabilistic prediction.\n",
    "- They are often easily interpretable.\n",
    "- They have few (if any) tunable parameters.\n",
    "\n",
    "These advantages mean a naive Bayes classifier is often a good choice as an initial baseline classification.\n",
    "\n",
    "!!!!!!!! A réécrire !!!!!!!\n",
    "The final two statements appear to be unrelated, but they are: as the dimensionality of a dataset increases, it becomes far less likely that any two points will be located near together (after all, they must be close in *every single dimension* to be close overall).\n",
    "This suggests that clusters in high dimensions are more separated than clusters in low dimensions on average, provided the new dimensions add information.\n",
    "As a result, as the dimensionality increases, simplest classifiers like the ones mentioned here tend to perform as well as or better than more complicated classifiers: provided you have enough data, even a simple model can be very strong.\n",
    "!!!!!!!! A réécrire !!!!!!!\n",
    "\n",
    "drawbacks :\n",
    "- If your test data set contains a categorical variable from a category that was not present in the training data set, the Naive Bayes model will assign it zero probability and will be unable to make predictions. This phenomena is known as 'Zero Frequency,' and it must be solved using a smoothing process.\n",
    "- This approach is also known for being a poor estimator. As a result, you shouldn't take the 'predict_proba' probability results too seriously. \n",
    "- It is assumed that all of the features are self-contained. While it may seem excellent in principle, it is rare to find a collection of independent traits in practice. \n",
    "\n",
    "## II) Python implementation\n",
    "\n",
    "As this model is just a baseline, we won't go into the details of Grid Research or Cross Validation. Indeed, this model will never be used in the future, as our data set is so imbalanced that certain probabilities are likely to be zero if they are not in the train set.\n",
    "\n",
    "### 1) Importing and setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "PATH = r'C:\\Users\\Thomas Aujoux\\Documents\\GitHub\\food-classification\\Data_Preprocessing\\data_clean\\clean.csv'\n",
    "\n",
    "def model_best_param(PATH, model, params):\n",
    "    \n",
    "    df = pd.read_csv(PATH)\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    df = df.drop(labels=22426, axis=0)\n",
    "\n",
    "    y = df[[\"Secteur\"]]\n",
    "    df_features = df.drop([\"Code_produit\", \"Secteur\", \"Famille\"], axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "    pipe_nb = make_pipeline(\n",
    "    model()\n",
    "    )\n",
    "    nbModel_grid = GridSearchCV(estimator=CategoricalNB(), param_grid=params, verbose=2, cv=5, n_jobs=-1)\n",
    "    nbModel_grid.fit(X_train, y_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code_produit</th>\n",
       "      <th>Secteur</th>\n",
       "      <th>Famille</th>\n",
       "      <th>abricot</th>\n",
       "      <th>abricots</th>\n",
       "      <th>acidifiant</th>\n",
       "      <th>acidifiant acidifiant</th>\n",
       "      <th>acidifiant antioxydant</th>\n",
       "      <th>acidifiant arome</th>\n",
       "      <th>acidifiant arome naturel</th>\n",
       "      <th>...</th>\n",
       "      <th>vitamine pp</th>\n",
       "      <th>vitamine vitamine</th>\n",
       "      <th>vitamine vitamine b</th>\n",
       "      <th>vitamine vitamine vitamine</th>\n",
       "      <th>vitamines</th>\n",
       "      <th>vitamines vitamine</th>\n",
       "      <th>vitamines vitamine vitamine</th>\n",
       "      <th>volaille</th>\n",
       "      <th>yaourt</th>\n",
       "      <th>yaourt brass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Yaourts et laits fermentes sucres classiques</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125377</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>453.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Yaourts et laits fermentes sucres classiques</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245026</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>455.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Yaourts et laits fermentes sucres classiques</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>456.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Yaourts et laits fermentes sucres classiques</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>460.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Fromages frais nature non sucres gourmands</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code_produit                              Secteur  \\\n",
       "0         450.0  Produits laitiers et desserts frais   \n",
       "1         453.0  Produits laitiers et desserts frais   \n",
       "2         455.0  Produits laitiers et desserts frais   \n",
       "3         456.0  Produits laitiers et desserts frais   \n",
       "4         460.0  Produits laitiers et desserts frais   \n",
       "\n",
       "                                        Famille  abricot  abricots  \\\n",
       "0  Yaourts et laits fermentes sucres classiques      0.0       0.0   \n",
       "1  Yaourts et laits fermentes sucres classiques      0.0       0.0   \n",
       "2  Yaourts et laits fermentes sucres classiques      0.0       0.0   \n",
       "3  Yaourts et laits fermentes sucres classiques      0.0       0.0   \n",
       "4    Fromages frais nature non sucres gourmands      0.0       0.0   \n",
       "\n",
       "   acidifiant  acidifiant acidifiant  acidifiant antioxydant  \\\n",
       "0         0.0                    0.0                     0.0   \n",
       "1         0.0                    0.0                     0.0   \n",
       "2         0.0                    0.0                     0.0   \n",
       "3         0.0                    0.0                     0.0   \n",
       "4         0.0                    0.0                     0.0   \n",
       "\n",
       "   acidifiant arome  acidifiant arome naturel  ...  vitamine pp  \\\n",
       "0               0.0                       0.0  ...          0.0   \n",
       "1               0.0                       0.0  ...          0.0   \n",
       "2               0.0                       0.0  ...          0.0   \n",
       "3               0.0                       0.0  ...          0.0   \n",
       "4               0.0                       0.0  ...          0.0   \n",
       "\n",
       "   vitamine vitamine  vitamine vitamine b  vitamine vitamine vitamine  \\\n",
       "0                0.0                  0.0                         0.0   \n",
       "1                0.0                  0.0                         0.0   \n",
       "2                0.0                  0.0                         0.0   \n",
       "3                0.0                  0.0                         0.0   \n",
       "4                0.0                  0.0                         0.0   \n",
       "\n",
       "   vitamines  vitamines vitamine  vitamines vitamine vitamine  volaille  \\\n",
       "0        0.0                 0.0                          0.0       0.0   \n",
       "1        0.0                 0.0                          0.0       0.0   \n",
       "2        0.0                 0.0                          0.0       0.0   \n",
       "3        0.0                 0.0                          0.0       0.0   \n",
       "4        0.0                 0.0                          0.0       0.0   \n",
       "\n",
       "     yaourt  yaourt brass  \n",
       "0  0.125377           0.0  \n",
       "1  0.245026           0.0  \n",
       "2  0.241945           0.0  \n",
       "3  0.242882           0.0  \n",
       "4  0.000000           0.0  \n",
       "\n",
       "[5 rows x 1503 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\Thomas Aujoux\\Documents\\GitHub\\food-classification\\Data_Preprocessing\\data_clean\\clean.csv')\n",
    "#df = df.drop('Unnamed: 3', axis=1)\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.drop(labels=22426, axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"Secteur\"]]\n",
    "df_features = df.drop([\"Code_produit\", \"Secteur\", \"Famille\"], axis=1)\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train, test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\n",
    "}\n",
    "\n",
    "model = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'var_smoothing' for estimator Pipeline(steps=[('multinomialnb', MultinomialNB())]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m pipe_nb \u001b[39m=\u001b[39m make_pipeline(\n\u001b[0;32m      9\u001b[0m     MultinomialNB()\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m random_search \u001b[39m=\u001b[39m RandomizedSearchCV(pipe_nb, param_grid_nb, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,  n_iter\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m random_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1805\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1806\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1807\u001b[0m         ParameterSampler(\n\u001b[0;32m   1808\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1809\u001b[0m         )\n\u001b[0;32m   1810\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1854\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1855\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1857\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1859\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1784\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1785\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1786\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:720\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m parameters\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    718\u001b[0m         cloned_parameters[k] \u001b[39m=\u001b[39m clone(v, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 720\u001b[0m     estimator \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcloned_parameters)\n\u001b[0;32m    722\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    724\u001b[0m X_train, y_train \u001b[39m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\pipeline.py:215\u001b[0m, in \u001b[0;36mPipeline.set_params\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_params\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    197\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Set the parameters of this estimator.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[39m    Valid parameter keys can be listed with ``get_params()``. Note that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39m        Pipeline class instance.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_params(\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\metaestimators.py:68\u001b[0m, in \u001b[0;36m_BaseComposition._set_params\u001b[1;34m(self, attr, **params)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_estimator(attr, name, params\u001b[39m.\u001b[39mpop(name))\n\u001b[0;32m     67\u001b[0m \u001b[39m# 3. Step parameters and other initialisation arguments\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:229\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_params:\n\u001b[0;32m    228\u001b[0m     local_valid_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_param_names()\n\u001b[1;32m--> 229\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    230\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid parameter \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m for estimator \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValid parameters are: \u001b[39m\u001b[39m{\u001b[39;00mlocal_valid_params\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m delim:\n\u001b[0;32m    235\u001b[0m     nested_params[key][sub_key] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'var_smoothing' for estimator Pipeline(steps=[('multinomialnb', MultinomialNB())]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "param_grid_nb = {\n",
    "    'var_smoothing': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_nb = make_pipeline(\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe_nb, param_grid_nb, cv=3, verbose=2,  n_iter=5)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Aujoux\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Thomas Aujoux\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CategoricalNB(alpha=0.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "params = {'alpha': [0.1],\n",
    "         }\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "nbModel_grid = GridSearchCV(estimator=CategoricalNB(), param_grid=params, verbose=2, cv=5, n_jobs=-1)\n",
    "nbModel_grid.fit(X_train, y_train)\n",
    "print(nbModel_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Aujoux\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CategoricalNB(alpha=0.1, fit_prior=False, min_categories=18)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],\n",
    "          'fit_prior': [True, False],\n",
    "          'min_categories': [18, 25, 30],\n",
    "          'class_prior': [None, [0.1,]* 31,]\n",
    "         }\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "nbModel_grid = GridSearchCV(estimator=CategoricalNB(), param_grid=params, verbose=2, cv=2, n_jobs=-1)\n",
    "nbModel_grid.fit(X_train, y_train)\n",
    "print(nbModel_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nbModel_grid\u001b[39m.\u001b[39;49mscore(X_test, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:471\u001b[0m, in \u001b[0;36mBaseSearchCV.score\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[39mreturn\u001b[39;00m scorer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_, X, y)\n\u001b[0;32m    470\u001b[0m \u001b[39m# callable\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscorer_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_, X, y)\n\u001b[0;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultimetric_:\n\u001b[0;32m    473\u001b[0m     score \u001b[39m=\u001b[39m score[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_scorer.py:527\u001b[0m, in \u001b[0;36m_PassthroughScorer.__call__\u001b[1;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    526\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Method that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[1;32m--> 527\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39;49mscore(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:705\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 705\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\naive_bayes.py:102\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    100\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    101\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(X)\n\u001b[1;32m--> 102\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joint_log_likelihood(X)\n\u001b[0;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\naive_bayes.py:1526\u001b[0m, in \u001b[0;36mCategoricalNB._joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_):\n\u001b[0;32m   1525\u001b[0m     indices \u001b[39m=\u001b[39m X[:, i]\n\u001b[1;32m-> 1526\u001b[0m     jll \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_log_prob_[i][:, indices]\u001b[39m.\u001b[39mT\n\u001b[0;32m   1527\u001b[0m total_ll \u001b[39m=\u001b[39m jll \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_log_prior_\n\u001b[0;32m   1528\u001b[0m \u001b[39mreturn\u001b[39;00m total_ll\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "nbModel_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 152    0    0    1    0    0    0    0    0   24    0    0    0    0\n",
      "     0    3    0    0    0    4    0    1    1    1    0    2    1    0\n",
      "     0    0    0]\n",
      " [   0  297    0    8    0    0    0    0    2    1    0    0    0    1\n",
      "     0    0    0    0   16    0    0    0   10    0    0    2    0    0\n",
      "     0    0    0]\n",
      " [   0    0   67    0    0    0    5    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    1 1298    1    0    0    0    4    0    0    0    0    2\n",
      "     0    0    0    0    7    0    0    0   17    2    1    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0  368    0    0    0    0    1    1    0    0    0\n",
      "     0   39    0    0    0    0    0    0    5    5    0    0    0    0\n",
      "     1    0    0]\n",
      " [   0    0    0    0    1  263    0    0    0    0    0    0    0    0\n",
      "     0    1    0    0    0    2    0    3    0    0    2    5    2    0\n",
      "     0    0    0]\n",
      " [   0    1    5    4    0    0  258    0    6    0    0    0    0    0\n",
      "     0    0    0    0    2    0    0    0    8    1    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0  532    0    0    0    0    0    0\n",
      "     0    0    0    0    0    1    0    0    4    1    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    1    0    1    0    0    1    0  292    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    7    1    0    0    0    0\n",
      "     0    0    1]\n",
      " [   1    0    0    1    0    0    0    0    0  353    0    2    3    0\n",
      "     0    1    0    0    0    0    0    0    2    3    0    0    0    0\n",
      "     0    0    2]\n",
      " [   0    0    0    0    0    0    0    0    7    0  204    0    0    0\n",
      "     2    8    0    0    0    0    0    0    7    1    0    0    0    0\n",
      "     7    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  257    2    0\n",
      "     0    1    0    0    0    0    0    0    0    3    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    1    0    1   99    0\n",
      "     0    9    0    0    0    0    0    0    1    7    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0  349\n",
      "     0    5    0    0    0    0    0    1    6   11    1    0    0    0\n",
      "     0    1    0]\n",
      " [   0    0    1    1    1    0    0    0    3    3    0    0    2    0\n",
      "   609    6    0    0    0    0    0    0    7    7    0    0    0    0\n",
      "     0    0    1]\n",
      " [   1    0    0    0    5    0    0    0    0    2    0    0    1    0\n",
      "     0  443    0    0    0    0    0    0    2    2    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0   27    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    1    0   42    1    0    0    0    1    1    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    1    1   23    0    0    1    0    1    0    0    0    0    0\n",
      "     0    1    0    0  586    0    0    1   16    1    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    1    0    0    0    0    0    2    0    0    0    0    0    1\n",
      "     0    2    0    0    0  891    3   16   14    0    2    5   12    1\n",
      "     0    0    0]\n",
      " [   0    1    0    3    0    0    0    2    0    1    0    0    0    1\n",
      "     0    0    0    0    0    5  376   31    3    1    3    2    0    0\n",
      "     0    1    0]\n",
      " [   0    0    0    0    0    2    0    5    0    0    0    0    0    1\n",
      "     0    2    0    0    6    8   23 1158    4    0    5   19    3    1\n",
      "     0    2    0]\n",
      " [   0    0    0   14    0    0    0    0    1    0    0    0    1    0\n",
      "     0    0    0    0    0    0    0    0   90    2    1    0    0    0\n",
      "     0    0    1]\n",
      " [   0    0    0    5    0    0    0    0    2    0    2    2    0    5\n",
      "     1    3    0    2    2    0    0    0   10 1154    0    0    0    1\n",
      "     0    0    0]\n",
      " [   0    0    0   13    0    0    0    1    0    1    0    0    0    2\n",
      "     0    5    0    1   12    3    0    8   21    6  586    1    2    8\n",
      "     0    9    1]\n",
      " [   0    3    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    1    0    0    0    0    0    1    3    0    0  286    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    1    0    0    0    0    0    0    0    0\n",
      "     0    1    0    0    0    2    0    1   13    4    0    0  150    1\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    1    0    1    0   10  238\n",
      "     0    0    0]\n",
      " [   0    0    0    0    4    0    0    0    0    0    0    0    0    0\n",
      "     0   28    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   140    0    0]\n",
      " [   0    0    0    1    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    2    0    1   11    0    0   12    0    0    0\n",
      "     0  422    1]\n",
      " [   0    0    0   16    0    0    0    0    2    1    0    0    0    0\n",
      "     0    0    0    0    8    0    0    0   10    1    0    0    0    1\n",
      "     0    0  153]] : is the confusion matrix\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'average'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(confusion_matrix(y_test, y_pred), \u001b[39m\"\u001b[39m\u001b[39m: is the confusion matrix\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(y_test, y_pred, average\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mweighted\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39m: is the accuracy score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m precision_score\n\u001b[0;32m      9\u001b[0m \u001b[39m#print(precision_score(y_test, y_pred), \": is the precision score\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:175\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[0;32m    174\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m params \u001b[39m=\u001b[39m func_sig\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    176\u001b[0m params\u001b[39m.\u001b[39mapply_defaults()\n\u001b[0;32m    178\u001b[0m \u001b[39m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:3211\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   3207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3208\u001b[0m \u001b[39m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m \u001b[39m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3210\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3211\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bind(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:3200\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3198\u001b[0m         arguments[kwargs_param\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m kwargs\n\u001b[0;32m   3199\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3200\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   3201\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m{arg!r}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   3202\u001b[0m                 arg\u001b[39m=\u001b[39m\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))))\n\u001b[0;32m   3204\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_arguments_cls(\u001b[39mself\u001b[39m, arguments)\n",
      "\u001b[1;31mTypeError\u001b[0m: got an unexpected keyword argument 'average'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = nbModel_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred), \": is the confusion matrix\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred, average=\"weighted\"), \": is the accuracy score\")\n",
    "from sklearn.metrics import precision_score\n",
    "#print(precision_score(y_test, y_pred), \": is the precision score\")\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_test, y_pred, average=\"weighted\"), \": is the recall score\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred, average=\"weighted\"), \": is the f1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9304107909258124 : is the recall score\n",
      "0.9329957886293471 : is the f1 score\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score\n",
    "#print(precision_score(y_test, y_pred), \": is the precision score\")\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_test, y_pred, average=\"weighted\"), \": is the recall score\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred, average=\"weighted\"), \": is the f1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Produits traiteurs frais', 'Plats cuisines ambiants',\n",
       "       'Plats cuisines ambiants', ..., 'Plats cuisines frais',\n",
       "       'Plats cuisines surgeles', 'Snacking surgele'], dtype='<U45')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb = clf_mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = clf_mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Aujoux\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m clf_mnb \u001b[39m=\u001b[39m CategoricalNB()\n\u001b[0;32m      3\u001b[0m clf_mnb \u001b[39m=\u001b[39m clf_mnb\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m----> 4\u001b[0m y_pred_mnb \u001b[39m=\u001b[39m clf_mnb\u001b[39m.\u001b[39;49mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\naive_bayes.py:102\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    100\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    101\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(X)\n\u001b[1;32m--> 102\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joint_log_likelihood(X)\n\u001b[0;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\naive_bayes.py:1526\u001b[0m, in \u001b[0;36mCategoricalNB._joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_):\n\u001b[0;32m   1525\u001b[0m     indices \u001b[39m=\u001b[39m X[:, i]\n\u001b[1;32m-> 1526\u001b[0m     jll \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_log_prob_[i][:, indices]\u001b[39m.\u001b[39mT\n\u001b[0;32m   1527\u001b[0m total_ll \u001b[39m=\u001b[39m jll \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_log_prior_\n\u001b[0;32m   1528\u001b[0m \u001b[39mreturn\u001b[39;00m total_ll\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf_mnb = CategoricalNB()\n",
    "clf_mnb = clf_mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = clf_mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Glaces et sorbets', 'Sauces condimentaires',\n",
       "       'Produits laitiers et desserts frais', ..., 'Glaces et sorbets',\n",
       "       'Produits laitiers et desserts frais',\n",
       "       'Produits transformes a base de pomme de terre'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred_mnb, labels=y_train.values)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=y_train.values, yticklabels=y_train.values)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,ConfusionMatrixDisplay,f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8501519756838906\n",
      "F1 Score: 0.8525029721038722\n"
     ]
    }
   ],
   "source": [
    "accuray = accuracy_score(y_pred_mnb, y_test)\n",
    "f1 = f1_score(y_pred_mnb, y_test, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy:\", accuray)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predict_category('sending a payload to the ISS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Aujoux\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.72036474164133\n",
      "F1 Score: 78.14901526262827\n",
      "Precision: 83.41057508069942\n",
      "Recall: 77.00806782690213\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "       Aliments infantiles de diversification       0.90      0.58      0.71       184\n",
      "                          Aperitifs a croquer       0.82      0.75      0.78       332\n",
      "                           Barres cerealieres       0.97      0.45      0.62        64\n",
      "              Biscuits et gateaux industriels       0.83      0.96      0.89      1346\n",
      "        Boissons Rafraichissantes Sans Alcool       0.84      0.87      0.85       450\n",
      "                         Bouillons et potages       0.89      0.71      0.79       267\n",
      "              Cereales pour le petit dejeuner       0.68      0.95      0.80       255\n",
      "                                  Charcuterie       0.89      0.97      0.93       547\n",
      "              Chocolat et produits chocolates       0.87      0.88      0.87       332\n",
      "                                     Compotes       0.93      0.96      0.94       387\n",
      "                                  Confiseries       0.86      0.79      0.82       253\n",
      "                                   Confitures       0.96      0.97      0.97       253\n",
      "                          Conserves de fruits       0.95      0.84      0.89       111\n",
      "                                     Fromages       0.96      0.95      0.95       354\n",
      "                            Glaces et sorbets       0.90      0.94      0.92       646\n",
      "                               Jus et nectars       0.89      0.91      0.90       503\n",
      "                             Laits infantiles       0.47      0.33      0.39        21\n",
      "                                   Margarines       0.51      0.97      0.67        34\n",
      "      Panification croustillante et moelleuse       0.89      0.84      0.87       651\n",
      "                      Plats cuisines ambiants       0.71      0.78      0.74       912\n",
      "                         Plats cuisines frais       0.73      0.37      0.50       468\n",
      "                      Plats cuisines surgeles       0.67      0.85      0.75      1192\n",
      "                   Preparations pour desserts       1.00      0.29      0.45       113\n",
      "          Produits laitiers et desserts frais       0.94      0.92      0.93      1224\n",
      "                     Produits traiteurs frais       0.86      0.58      0.69       685\n",
      "Produits transformes a base de pomme de terre       0.91      0.93      0.92       295\n",
      "                               Sauces chaudes       0.91      0.47      0.62       180\n",
      "                        Sauces condimentaires       0.82      0.94      0.87       250\n",
      "      Sirops et boissons concentrees a diluer       0.89      0.74      0.81       198\n",
      "                             Snacking surgele       0.69      0.77      0.73       441\n",
      "           Viennoiseries et desserts surgeles       0.74      0.60      0.66       212\n",
      "\n",
      "                                     accuracy                           0.83     13160\n",
      "                                    macro avg       0.83      0.77      0.78     13160\n",
      "                                 weighted avg       0.84      0.83      0.82     13160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,ConfusionMatrixDisplay,f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "#Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred)*100)\n",
    "print(\"F1 Score:\",f1_score(y_test, y_pred, average=\"macro\")*100)\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average=\"macro\")*100)\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average=\"macro\")*100)\n",
    "print (classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function that handles sample splitting, model fitting and report printing \n",
    "def mfunc(X, y, typ):\n",
    "    \n",
    "    # Create training and testing samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Fit the model\n",
    "    model = typ\n",
    "    clf = model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict class labels on a test data\n",
    "    pred_labels = model.predict(X_test)\n",
    "\n",
    "    # Print model attributes \n",
    "    print('Classes: ', clf.classes_) # class labels known to the classifier\n",
    "    if str(typ)=='GaussianNB()':\n",
    "        print('Class Priors: ',clf.class_prior_) # prior probability of each class.\n",
    "    else: \n",
    "        print('Class Log Priors: ',clf.class_log_prior_) # log prior probability of each class.\n",
    "        \n",
    "    # Use score method to get accuracy of the model\n",
    "    print('--------------------------------------------------------')\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Accuracy Score: ', score)\n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    # Look at classification report to evaluate the model\n",
    "    print(classification_report(y_test, pred_labels))\n",
    "    \n",
    "    # Return relevant data for chart plotting\n",
    "    return X_train, X_test, y_train, y_test, clf, pred_labels\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Aujoux\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test, clf, pred_labels \u001b[39m=\u001b[39m mfunc(X_train, y_train, CategoricalNB())\n",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m, in \u001b[0;36mmfunc\u001b[1;34m(X, y, typ)\u001b[0m\n\u001b[0;32m      9\u001b[0m clf \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Predict class labels on a test data\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m pred_labels \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Print model attributes \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mClasses: \u001b[39m\u001b[39m'\u001b[39m, clf\u001b[39m.\u001b[39mclasses_) \u001b[39m# class labels known to the classifier\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\naive_bayes.py:102\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    100\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    101\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(X)\n\u001b[1;32m--> 102\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joint_log_likelihood(X)\n\u001b[0;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\naive_bayes.py:1526\u001b[0m, in \u001b[0;36mCategoricalNB._joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_):\n\u001b[0;32m   1525\u001b[0m     indices \u001b[39m=\u001b[39m X[:, i]\n\u001b[1;32m-> 1526\u001b[0m     jll \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_log_prob_[i][:, indices]\u001b[39m.\u001b[39mT\n\u001b[0;32m   1527\u001b[0m total_ll \u001b[39m=\u001b[39m jll \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_log_prior_\n\u001b[0;32m   1528\u001b[0m \u001b[39mreturn\u001b[39;00m total_ll\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, clf, pred_labels = mfunc(X_train, y_train, CategoricalNB())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
