{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multunomiale Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Theory\n",
    "\n",
    "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
    "Because they are so fast and have so few tunable parameters, they end up being useful as a quick-and-dirty baseline for a classification problem.\n",
    "\n",
    "### 1) Equations\n",
    "\n",
    "Naive Bayes classifiers are built on Bayesian classification methods.\n",
    "These rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities.\n",
    "In Bayesian classification, we're interested in finding the probability of a label $L$ given some observed features, which we can write as $P(L~|~{\\rm features})$.\n",
    "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "$$\n",
    "P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}\n",
    "$$\n",
    "\n",
    "If we are trying to decide between two labels—let's call them $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "\n",
    "$$\n",
    "\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n",
    "$$\n",
    "\n",
    "All we need now is some model by which we can compute $P({\\rm features}~|~L_i)$ for each label.\n",
    "Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
    "Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
    "The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
    "\n",
    "This is where the \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.\n",
    "Different types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine a few of these in the following sections.\n",
    "\n",
    "### 2) Multinomial Naive Bayes\n",
    "\n",
    "The features are assumed to be generated from a simple multinomial distribution.\n",
    "The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\n",
    "\n",
    "One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified.\n",
    "We discussed the extraction of such features from text in [Feature Engineering](05.04-Feature-Engineering.ipynb); here we will use the sparse word count features from the 20 Newsgroups corpus made available through Scikit-Learn to show how we might classify these short documents into categories.\n",
    "\n",
    "In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers.\n",
    "For this we will use the TF-IDF vectorizer (introduced in [Feature Engineering](05.04-Feature-Engineering.ipynb)), and create a pipeline that attaches it to a multinomial naive Bayes classifier:\n",
    "\n",
    "## When to Use Naive Bayes\n",
    "\n",
    "Because naive Bayes classifiers make such stringent assumptions about data, they will generally not perform as well as more complicated models.\n",
    "That said, they have several advantages:\n",
    "\n",
    "- They are fast for both training and prediction.\n",
    "- They provide straightforward probabilistic prediction.\n",
    "- They are often easily interpretable.\n",
    "- They have few (if any) tunable parameters.\n",
    "\n",
    "These advantages mean a naive Bayes classifier is often a good choice as an initial baseline classification.\n",
    "If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem.\n",
    "If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\n",
    "\n",
    "Naive Bayes classifiers tend to perform especially well in the following situations:\n",
    "\n",
    "- When the naive assumptions actually match the data (very rare in practice)\n",
    "- For very well-separated categories, when model complexity is less important\n",
    "- For very high-dimensional data, when model complexity is less important\n",
    "\n",
    "The last two points seem distinct, but they actually are related: as the dimensionality of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in *every single dimension* to be close overall).\n",
    "This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information.\n",
    "For this reason, simplistic classifiers like the ones discussed here tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful.\n",
    "\n",
    "### 3) Conclusion\n",
    "\n",
    "Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator.\n",
    "For example, let's take a look at the confusion matrix between the true and predicted labels for the test data (see the following figure):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Python implementation\n",
    "\n",
    "### 1) Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code_produit</th>\n",
       "      <th>Secteur</th>\n",
       "      <th>Ingrédient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>lait_ecreme_reconstitue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>450</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>sucre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>450</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>450</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>creme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>450</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>epaississants</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code_produit                              Secteur               Ingrédient\n",
       "0           450  Produits laitiers et desserts frais  lait_ecreme_reconstitue\n",
       "1           450  Produits laitiers et desserts frais                    sucre\n",
       "2           450  Produits laitiers et desserts frais                    fruit\n",
       "3           450  Produits laitiers et desserts frais                    creme\n",
       "4           450  Produits laitiers et desserts frais            epaississants"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\Thomas Aujoux\\Documents\\GitHub\\food-classification\\data_preparation\\clean_.csv')\n",
    "df = df[[\"Code_produit\", \"Secteur\", \"Ingrédient\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.groupby(['Code_produit', 'Secteur'])['Ingrédient'].agg(lambda col: ' '.join(col)).reset_index(name='Ingrédient')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "coun_vect = CountVectorizer(token_pattern='\\\\b(\\\\w+[\\\\.,%_1234567890:()]?\\\\w+)\\\\b')\n",
    "count_matrix = coun_vect.fit_transform(df_new['Ingrédient'])\n",
    "count_array = count_matrix.toarray()\n",
    "df_vect = pd.DataFrame(data=count_array,columns = coun_vect.get_feature_names_out())\n",
    "new_df = pd.concat([df_new[[\"Code_produit\", \"Secteur\"]], df_vect], axis=1)\n",
    "new_df = new_df.drop(['_ananas', '_b1,_b9', '_b1,_b9_et_provitamine_a', '_b12', '_caillettes_de_boeuf_en_proportions_naturelles', '_carotte,_potiron_et_spiruline', '_d_avoine,_de_riz_et_d_epeautre', '_d_orge_et_de_riz', '_de_cassis_et_de_mure', '_de_cuivre', '_de_cuivre_et_de_manganese', '_de_matiere_grasse', '_de_matieres_grasses_au_lait_demi_ecreme_pasteurise', '_de_mg', '_de_thym,_de_laurier', '_e', '_gr', '_lb.rhamnosus', '_maigre_de_tete,_viande_et_couenne_de_porc', '_mg', '_paprika', '_provitamine_a', '_rooibos_et_du_melange_de_13_plantes_ricola', '_tournesol', '_vanille'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code_produit</th>\n",
       "      <th>Secteur</th>\n",
       "      <th>3_poivres</th>\n",
       "      <th>abondance_aop</th>\n",
       "      <th>abricot</th>\n",
       "      <th>abricot_de_provence</th>\n",
       "      <th>abricot_deshydrate</th>\n",
       "      <th>abricot_lyophilise</th>\n",
       "      <th>abricot_nectarine</th>\n",
       "      <th>abricot_orange</th>\n",
       "      <th>...</th>\n",
       "      <th>zestes_d_orange</th>\n",
       "      <th>zestes_d_oranges</th>\n",
       "      <th>zestes_de_citron</th>\n",
       "      <th>zestes_de_citron_deshydrates</th>\n",
       "      <th>zestes_de_citron_jaune</th>\n",
       "      <th>zestes_de_citron_vert</th>\n",
       "      <th>zestes_de_citrons</th>\n",
       "      <th>zestes_de_mandarine</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zygochlamys_patagonica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>453</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>455</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>456</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>460</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12763 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code_produit                              Secteur  3_poivres  \\\n",
       "0           450  Produits laitiers et desserts frais          0   \n",
       "1           453  Produits laitiers et desserts frais          0   \n",
       "2           455  Produits laitiers et desserts frais          0   \n",
       "3           456  Produits laitiers et desserts frais          0   \n",
       "4           460  Produits laitiers et desserts frais          0   \n",
       "\n",
       "   abondance_aop  abricot  abricot_de_provence  abricot_deshydrate  \\\n",
       "0              0        0                    0                   0   \n",
       "1              0        0                    0                   0   \n",
       "2              0        0                    0                   0   \n",
       "3              0        0                    0                   0   \n",
       "4              0        0                    0                   0   \n",
       "\n",
       "   abricot_lyophilise  abricot_nectarine  abricot_orange  ...  \\\n",
       "0                   0                  0               0  ...   \n",
       "1                   0                  0               0  ...   \n",
       "2                   0                  0               0  ...   \n",
       "3                   0                  0               0  ...   \n",
       "4                   0                  0               0  ...   \n",
       "\n",
       "   zestes_d_orange  zestes_d_oranges  zestes_de_citron  \\\n",
       "0                0                 0                 0   \n",
       "1                0                 0                 0   \n",
       "2                0                 0                 0   \n",
       "3                0                 0                 0   \n",
       "4                0                 0                 0   \n",
       "\n",
       "   zestes_de_citron_deshydrates  zestes_de_citron_jaune  \\\n",
       "0                             0                       0   \n",
       "1                             0                       0   \n",
       "2                             0                       0   \n",
       "3                             0                       0   \n",
       "4                             0                       0   \n",
       "\n",
       "   zestes_de_citron_vert  zestes_de_citrons  zestes_de_mandarine  zinc  \\\n",
       "0                      0                  0                    0     0   \n",
       "1                      0                  0                    0     0   \n",
       "2                      0                  0                    0     0   \n",
       "3                      0                  0                    0     0   \n",
       "4                      0                  0                    0     0   \n",
       "\n",
       "   zygochlamys_patagonica  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "\n",
       "[5 rows x 12763 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df[\"Secteur\"]\n",
    "df_features = new_df.drop([\"Code_produit\", \"Secteur\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=30)\n",
    "X_lda = lda.fit_transform(df_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_var = sum(lda.explained_variance_ratio_ * 100)\n",
    "print('Variance explained:', exp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=None)\n",
    "X_lda = lda.fit(df_features, y)\n",
    "\n",
    "exp_var = lda.explained_variance_ratio_ * 100\n",
    "cum_exp_var = np.cumsum(exp_var)\n",
    "\n",
    "plt.bar(range(1, 32), exp_var, align='center',\n",
    "        label='Individual explained variance')\n",
    "\n",
    "plt.step(range(1, 32), cum_exp_var, where='mid',\n",
    "         label='Cumulative explained variance', color='red')\n",
    "\n",
    "plt.ylabel('Explained variance percentage')\n",
    "plt.xlabel('Component index')\n",
    "plt.xticks(ticks=[1, 2])\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Barplot_LDA.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=[7, 5])\n",
    "\n",
    "plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, s=25, cmap='plasma')\n",
    "plt.title('LDA for wine data with 2 components')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.savefig(\"LDA.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datascience.stackexchange.com/questions/30912/what-does-baseline-mean-in-the-context-of-machine-learning\n",
    "\n",
    "A baseline is a method that uses heuristics, simple summary statistics, randomness, or machine learning to create predictions for a dataset. You can use these predictions to measure the baseline's performance (e.g., accuracy)-- this metric will then become what you compare any other machine learning algorithm against.\n",
    "\n",
    "\n",
    "A baseline is a method that uses heuristics, simple summary statistics, randomness, or machine learning to create predictions for a dataset. You can use these predictions to measure the baseline's performance (e.g., accuracy)-- this metric will then become what you compare any other machine learning algorithm against.\n",
    "\n",
    "In more detail:\n",
    "\n",
    "A machine learning algorithm tries to learn a function that models the relationship between the input (feature) data and the target variable (or label). When you test it, you will typically measure performance in one way or another. For example, your algorithm may be 75% accurate. But what does this mean? You can infer this meaning by comparing with a baseline's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Multinomial Naive Bayes\n",
    "\n",
    "The sklearn's naive bayes class has a module for multinomial models. It is designed for classification with discrete features (e.g. word counts for text classification) - perfect for this problem. With text classification, it requires integer feature counts or tf-idf input.\n",
    "\n",
    "Alpha is the only parameter that I have tuned. I tried [0.01, 0.02, 0.035, 0.04, 0.1, 0.5, 1] and found that 0.035 gives the highest validation score - 0.7424."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Best estimator after running the grid search cross validation for the Multinomial Naive Bayes model\n",
    "clf_mnb = MultinomialNB(alpha=0.035, class_prior=None, fit_prior=True)\n",
    "clf_mnb = clf_mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = clf_mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.374648</td>\n",
       "      <td>3.204410</td>\n",
       "      <td>0.845373</td>\n",
       "      <td>0.862802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.111102</td>\n",
       "      <td>3.578398</td>\n",
       "      <td>0.847576</td>\n",
       "      <td>0.863521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.113058</td>\n",
       "      <td>3.541667</td>\n",
       "      <td>0.842978</td>\n",
       "      <td>0.862970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.591593</td>\n",
       "      <td>3.344312</td>\n",
       "      <td>0.844591</td>\n",
       "      <td>0.862303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.013602</td>\n",
       "      <td>3.332628</td>\n",
       "      <td>0.842963</td>\n",
       "      <td>0.862925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_score  train_score\n",
       "0  48.374648    3.204410    0.845373     0.862802\n",
       "1  41.111102    3.578398    0.847576     0.863521\n",
       "2  42.113058    3.541667    0.842978     0.862970\n",
       "3  36.591593    3.344312    0.844591     0.862303\n",
       "4  37.013602    3.332628    0.842963     0.862925"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "pipe_nb = make_pipeline(\n",
    "    MultinomialNB()\n",
    ")\n",
    "scores = cross_validate(pipe_nb, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'alpha' for estimator Pipeline(steps=[('multinomialnb', MultinomialNB())]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m pipe_nb \u001b[39m=\u001b[39m make_pipeline(\n\u001b[0;32m      6\u001b[0m     MultinomialNB()\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m random_search \u001b[39m=\u001b[39m RandomizedSearchCV(pipe_nb, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,  n_iter\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m random_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1769\u001b[0m         ParameterSampler(\n\u001b[0;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1771\u001b[0m         )\n\u001b[0;32m   1772\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:674\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m parameters\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    672\u001b[0m         cloned_parameters[k] \u001b[39m=\u001b[39m clone(v, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 674\u001b[0m     estimator \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcloned_parameters)\n\u001b[0;32m    676\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    678\u001b[0m X_train, y_train \u001b[39m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:211\u001b[0m, in \u001b[0;36mPipeline.set_params\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_params\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    193\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Set the parameters of this estimator.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[39m    Valid parameter keys can be listed with ``get_params()``. Note that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39m        Pipeline class instance.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_params(\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py:70\u001b[0m, in \u001b[0;36m_BaseComposition._set_params\u001b[1;34m(self, attr, **params)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_estimator(attr, name, params\u001b[39m.\u001b[39mpop(name))\n\u001b[0;32m     69\u001b[0m \u001b[39m# 3. Step parameters and other initialisation arguments\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:205\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_params:\n\u001b[0;32m    204\u001b[0m     local_valid_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_param_names()\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid parameter \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m for estimator \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValid parameters are: \u001b[39m\u001b[39m{\u001b[39;00mlocal_valid_params\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m delim:\n\u001b[0;32m    211\u001b[0m     nested_params[key][sub_key] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'alpha' for estimator Pipeline(steps=[('multinomialnb', MultinomialNB())]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "param_grid_nb = {\n",
    "    'var_smoothing': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_nb = make_pipeline(\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe_nb, param_grid, cv=3, verbose=2,  n_iter=5)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "2 fits failed out of a total of 6.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 267, in fit\n",
      "    return self._partial_fit(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py\", line 436, in _partial_fit\n",
      "    self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()\n",
      "                                         ^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 200, in var\n",
      "  File \"c:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 3747, in var\n",
      "    return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py\", line 236, in _var\n",
      "    x = asanyarray(arr - arrmean)\n",
      "                   ~~~~^~~~~~~~~\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 4.13 GiB for an array with shape (43490, 12761) and data type float64\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(var_smoothing=1e-05)\n"
     ]
    }
   ],
   "source": [
    "param_grid_nb = {\n",
    "    'var_smoothing': [0.00001]\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=2, cv=6, n_jobs=-1)\n",
    "nbModel_grid.fit(X_train, y_train)\n",
    "print(nbModel_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.724095646842428"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbModel_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151   0   1   0   0   0   1   1   1  26   0   0   3   0   0   2   2   0\n",
      "    0   0   0   0   0   1   0   0   0   0   1   0   0]\n",
      " [  0 239   2   1   0   0   3   0  34   0   0   0   0   1   0   0   0   6\n",
      "   13   0   0   0  10   0   0  28   0   0   0   0   0]\n",
      " [  0   0  67   0   0   0   5   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2   7  28 774   0   0  10   0 112   1   1   0   0   0   9   0   0   1\n",
      "  156   0   0   0 196  25   1   2   0   0   0   0   8]\n",
      " [  0   0   0   0 363   0   0   1   0   6   1   1   8   0   0   5   0   1\n",
      "    0   0   0   0   3   2   0   1   0   0  28   0   0]\n",
      " [  6   2   0   0   0 255   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   3   1   0   0   0   1   4   5   2   0   0   0]\n",
      " [  0   1  28   1   0   0 238   0  12   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   5   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 532   0   0   0   0   0   3   0   0   0   0\n",
      "    0   0   0   0   0   0   2   0   0   1   0   0   0]\n",
      " [  2   0   1   4   0   0   1   0 245   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  50   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 349   1  10   6   0   0   0   0   0\n",
      "    1   0   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   2   0   0   1   7   1 178   8   5   0   1   0   0   0\n",
      "    0   0   0   0  25   0   0   1   0   0   6   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   7   0 254   2   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2 116   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   2   0   0   2   1   0   0   0   0   0   1   2 364   0   0   0   0\n",
      "    0   0   0   0   0   0   1   1   0   0   0   0   0]\n",
      " [  0   0   3   4  11   0   0   2  12   1   1   1   0   0 500   0   0   0\n",
      "    1   0   0   0  35  42   0   3   0   2   3   0  20]\n",
      " [  0   0   0   0  16   0   0   0   0  38   0  10 104   0   0 254   0   1\n",
      "    0   0   0   0   0   0   0   0   0   0  33   0   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  25   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0  45\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   8   4  14   0   0   7   0  10   3   0   0   3   2   0   0   0   0\n",
      "  498   0   0   0  70   0   0  12   0   0   0   0   1]\n",
      " [  9   4   1   0   0  34   1  11   0   1   1   1   0  22   0   0   0   1\n",
      "    7 607  12   2   4   0   5  70  84  70   3   0   0]\n",
      " [  3   9   0   0   0  13   0  11   0   2   0   0   2   1   2   0   0   0\n",
      "    3  26 279  15   5   2  19  25   9   2   0   2   0]\n",
      " [ 19  18   1   0   0 125   0  33   0   1   0   0   0  10   0   0   0   0\n",
      "   10  71 177 471   2   1  59 120  77  18   0  25   1]\n",
      " [  0   2   0   0   0   0   0   0   0   0   0   2   0   1   1   0   0   0\n",
      "    1   0   0   0 101   0   0   1   0   0   0   0   1]\n",
      " [  0   0   0  10   1   0   0   3   2  20   4   4  12 163   7   0   4   0\n",
      "    1   0   0   0  66 877   0   0   0   2   3   0  10]\n",
      " [  2   2   0   9   4   9   0   4   0   3   0   0   1  42   0   0   0   3\n",
      "   26   8   9   3  19   6 457  10  16  37   0   3   7]\n",
      " [  0   2   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0 291   0   0   0   0   0]\n",
      " [  0   1   0   0   0   6   0   0   0   0   0   0   5   5   0   1   0   0\n",
      "    0   2   0   0   0   0   0   8 135  10   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   2 248   0   0   0]\n",
      " [  0   0   0   0   6   0   0   0   0   0   0   0   3   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0 163   0   0]\n",
      " [  0   3   0   0   0  11   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "    2   8  15   9   1   0 143   4   9   0   0 243   1]\n",
      " [  0   1   6  10   0   0   0   0   3   0   0   0   0   0   2   0   0   0\n",
      "   16   0   0   0  17   5   0   2   0   0   0   1 129]] : is the confusion matrix\n",
      "0.724095646842428 : is the accuracy score\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(y_test, y_pred), \u001b[39m\"\u001b[39m\u001b[39m: is the accuracy score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m precision_score\n\u001b[1;32m----> 9\u001b[0m \u001b[39mprint\u001b[39m(precision_score(y_test, y_pred), \u001b[39m\"\u001b[39m\u001b[39m: is the precision score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m recall_score\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(recall_score(y_test, y_pred), \u001b[39m\"\u001b[39m\u001b[39m: is the recall score\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[0;32m   1826\u001b[0m     y_true,\n\u001b[0;32m   1827\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1834\u001b[0m ):\n\u001b[0;32m   1835\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1954\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1955\u001b[0m         y_true,\n\u001b[0;32m   1956\u001b[0m         y_pred,\n\u001b[0;32m   1957\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1958\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1959\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1960\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1961\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1962\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1963\u001b[0m     )\n\u001b[0;32m   1964\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1391\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1389\u001b[0m         \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1390\u001b[0m             average_options\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTarget is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but average=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1393\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchoose another average setting, one of \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1394\u001b[0m         )\n\u001b[0;32m   1395\u001b[0m \u001b[39melif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m):\n\u001b[0;32m   1396\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1397\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pos_label (set to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) is ignored when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maverage != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m). You may use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m   1402\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = nbModel_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred), \": is the confusion matrix\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred, average=\"weighted\"), \": is the accuracy score\")\n",
    "from sklearn.metrics import precision_score\n",
    "#print(precision_score(y_test, y_pred), \": is the precision score\")\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_test, y_pred, average=\"weighted\"), \": is the recall score\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred, average=\"weighted\"), \": is the f1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724095646842428 : is the recall score\n",
      "0.7305783319320042 : is the f1 score\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score\n",
    "#print(precision_score(y_test, y_pred), \": is the precision score\")\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_test, y_pred, average=\"weighted\"), \": is the recall score\")\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test, y_pred, average=\"weighted\"), \": is the f1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Produits traiteurs frais', 'Plats cuisines ambiants',\n",
       "       'Plats cuisines ambiants', ..., 'Plats cuisines frais',\n",
       "       'Plats cuisines surgeles', 'Snacking surgele'], dtype='<U45')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb = clf_mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = clf_mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m clf_mnb \u001b[39m=\u001b[39m CategoricalNB()\n\u001b[0;32m      3\u001b[0m clf_mnb \u001b[39m=\u001b[39m clf_mnb\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m----> 4\u001b[0m y_pred_mnb \u001b[39m=\u001b[39m clf_mnb\u001b[39m.\u001b[39;49mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:106\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    105\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(X)\n\u001b[1;32m--> 106\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joint_log_likelihood(X)\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:1530\u001b[0m, in \u001b[0;36mCategoricalNB._joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1528\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_):\n\u001b[0;32m   1529\u001b[0m     indices \u001b[39m=\u001b[39m X[:, i]\n\u001b[1;32m-> 1530\u001b[0m     jll \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_log_prob_[i][:, indices]\u001b[39m.\u001b[39mT\n\u001b[0;32m   1531\u001b[0m total_ll \u001b[39m=\u001b[39m jll \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_log_prior_\n\u001b[0;32m   1532\u001b[0m \u001b[39mreturn\u001b[39;00m total_ll\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf_mnb = CategoricalNB()\n",
    "clf_mnb = clf_mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = clf_mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Glaces et sorbets', 'Sauces condimentaires',\n",
       "       'Produits laitiers et desserts frais', ..., 'Glaces et sorbets',\n",
       "       'Produits laitiers et desserts frais',\n",
       "       'Produits transformes a base de pomme de terre'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred_mnb, labels=y_train.values)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=y_train.values, yticklabels=y_train.values)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,ConfusionMatrixDisplay,f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8923206621704476\n",
      "F1 Score: 0.8922367361212168\n"
     ]
    }
   ],
   "source": [
    "accuray = accuracy_score(y_pred_mnb, y_test)\n",
    "f1 = f1_score(y_pred_mnb, y_test, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy:\", accuray)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predict_category('sending a payload to the ISS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.33108522378909\n",
      "F1 Score: 81.30978080986513\n",
      "Precision: 89.2926639219838\n",
      "Recall: 78.49630124315333\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "       Aliments infantiles de diversification       0.98      0.65      0.78       190\n",
      "                          Aperitifs a croquer       0.94      0.79      0.85       337\n",
      "                           Barres cerealieres       1.00      0.47      0.64        72\n",
      "              Biscuits et gateaux industriels       0.82      0.98      0.89      1333\n",
      "        Boissons Rafraichissantes Sans Alcool       0.79      0.91      0.85       420\n",
      "                         Bouillons et potages       0.89      0.83      0.86       279\n",
      "              Cereales pour le petit dejeuner       0.83      0.93      0.88       285\n",
      "                                  Charcuterie       0.96      0.96      0.96       538\n",
      "              Chocolat et produits chocolates       0.98      0.83      0.90       304\n",
      "                                     Compotes       0.90      0.94      0.92       368\n",
      "                                  Confiseries       0.95      0.78      0.85       236\n",
      "                                   Confitures       0.94      0.98      0.96       263\n",
      "                          Conserves de fruits       0.99      0.78      0.87       118\n",
      "                                     Fromages       0.93      0.90      0.91       374\n",
      "                            Glaces et sorbets       0.86      0.97      0.91       641\n",
      "                               Jus et nectars       0.96      0.79      0.87       456\n",
      "                             Laits infantiles       0.93      0.93      0.93        27\n",
      "                                   Margarines       1.00      0.70      0.82        46\n",
      "      Panification croustillante et moelleuse       0.83      0.85      0.84       632\n",
      "                      Plats cuisines ambiants       0.83      0.89      0.86       950\n",
      "                         Plats cuisines frais       0.76      0.59      0.66       430\n",
      "                      Plats cuisines surgeles       0.74      0.87      0.80      1239\n",
      "                   Preparations pour desserts       1.00      0.04      0.07       110\n",
      "          Produits laitiers et desserts frais       0.92      0.93      0.92      1189\n",
      "                     Produits traiteurs frais       0.80      0.77      0.78       680\n",
      "Produits transformes a base de pomme de terre       0.98      0.81      0.88       294\n",
      "                               Sauces chaudes       0.90      0.44      0.59       173\n",
      "                        Sauces condimentaires       0.93      0.87      0.90       250\n",
      "      Sirops et boissons concentrees a diluer       0.88      0.60      0.72       172\n",
      "                             Snacking surgele       0.76      0.88      0.81       450\n",
      "           Viennoiseries et desserts surgeles       0.72      0.69      0.70       192\n",
      "\n",
      "                                     accuracy                           0.85     13048\n",
      "                                    macro avg       0.89      0.78      0.81     13048\n",
      "                                 weighted avg       0.86      0.85      0.85     13048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,ConfusionMatrixDisplay,f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "#Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_pred)*100)\n",
    "print(\"F1 Score:\",f1_score(y_test, y_pred, average=\"macro\")*100)\n",
    "print(\"Precision:\",precision_score(y_test, y_pred, average=\"macro\")*100)\n",
    "print(\"Recall:\",recall_score(y_test, y_pred, average=\"macro\")*100)\n",
    "print (classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function that handles sample splitting, model fitting and report printing \n",
    "def mfunc(X, y, typ):\n",
    "    \n",
    "    # Create training and testing samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Fit the model\n",
    "    model = typ\n",
    "    clf = model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict class labels on a test data\n",
    "    pred_labels = model.predict(X_test)\n",
    "\n",
    "    # Print model attributes \n",
    "    print('Classes: ', clf.classes_) # class labels known to the classifier\n",
    "    if str(typ)=='GaussianNB()':\n",
    "        print('Class Priors: ',clf.class_prior_) # prior probability of each class.\n",
    "    else: \n",
    "        print('Class Log Priors: ',clf.class_log_prior_) # log prior probability of each class.\n",
    "        \n",
    "    # Use score method to get accuracy of the model\n",
    "    print('--------------------------------------------------------')\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Accuracy Score: ', score)\n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    # Look at classification report to evaluate the model\n",
    "    print(classification_report(y_test, pred_labels))\n",
    "    \n",
    "    # Return relevant data for chart plotting\n",
    "    return X_train, X_test, y_train, y_test, clf, pred_labels\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test, clf, pred_labels \u001b[39m=\u001b[39m mfunc(X_train, y_train, CategoricalNB())\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mmfunc\u001b[1;34m(X, y, typ)\u001b[0m\n\u001b[0;32m      9\u001b[0m clf \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Predict class labels on a test data\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m pred_labels \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Print model attributes \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mClasses: \u001b[39m\u001b[39m'\u001b[39m, clf\u001b[39m.\u001b[39mclasses_) \u001b[39m# class labels known to the classifier\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:106\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    105\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(X)\n\u001b[1;32m--> 106\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joint_log_likelihood(X)\n\u001b[0;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:1530\u001b[0m, in \u001b[0;36mCategoricalNB._joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1528\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_):\n\u001b[0;32m   1529\u001b[0m     indices \u001b[39m=\u001b[39m X[:, i]\n\u001b[1;32m-> 1530\u001b[0m     jll \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_log_prob_[i][:, indices]\u001b[39m.\u001b[39mT\n\u001b[0;32m   1531\u001b[0m total_ll \u001b[39m=\u001b[39m jll \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_log_prior_\n\u001b[0;32m   1532\u001b[0m \u001b[39mreturn\u001b[39;00m total_ll\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 5"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, clf, pred_labels = mfunc(X_train, y_train, CategoricalNB())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
