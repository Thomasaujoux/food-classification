{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for classic Python operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import libraries for data pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Importing libraries for model selection\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importing libraries for results analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "# Importing the library for the model under test\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost \n",
    "\n",
    "## I) Theory\n",
    "\n",
    "XGBoost is a gradient boosting ensemble Machine Learning technique based on decision trees.\n",
    "We've already seen what the terms decision tree, bagging and random forest mean in the Random Forest section, so let's take a closer look at what's new in XGBoost.\n",
    "\n",
    "### 1) What does boosting mean ?\n",
    "\n",
    "Boosting is an ensemble strategy that adds new models to repair faults generated by current models. First, a model is constructed using the training data. The second model is then constructed in an attempt to address the faults in the previous model and models are added sequentially until no further advancements are possible. This ensemble method attempts to build a powerful classifier using previously 'weaker' classifiers.\n",
    "\n",
    "### 2) What does gradient means ?\n",
    "\n",
    "Gradient boosting is so named because it employs a gradient descent approach to minimize loss when adding new models.\n",
    "\n",
    "### 3) What does eXtreme mean\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an advanced implementation of a gradient boosting algorithm.\n",
    "\n",
    "## II) Advantages and Drawbacks\n",
    "\n",
    "Advantages\n",
    "\n",
    "-  XGBoost Execution Speed:\n",
    "XGBoost is generally quick compared to other gradient boosting implementations.\n",
    "\n",
    "- Performance: XGBoost has a strong track record of producing high-quality results in various machine learning tasks, especially in Kaggle competitions, where it has been a popular choice for winning solutions.\n",
    "\n",
    "- Regularization :\n",
    "Standard GBM implementation has no regularization like XGBoost; therefore, it also helps to reduce overfitting. In fact, XGBoost is also known as a ‘regularized boosting‘ technique.\n",
    "\n",
    "- High Flexibility:\n",
    "XGBoost allows users to define custom optimization objectives and evaluation criteria. This adds a whole new dimension to the model and there is no limit to what we can do.\n",
    "\n",
    "- Handling Missing Values:\n",
    "XGBoost has an in-built routine to handle missing values. The user is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in the future.\n",
    "\n",
    "- Interpretability: Unlike some machine learning algorithms that can be difficult to interpret, XGBoost provides feature importances, allowing for a better understanding of which variables are most important in making predictions.\n",
    "\n",
    "Drawbacks\n",
    "\n",
    "- Computational Complexity:\n",
    "XGBoost can be computationally intensive, especially when training large models, making it less suitable for resource-constrained systems.\n",
    "\n",
    "- Overfitting: XGBoost can be prone to overfitting, especially when trained on small datasets or when too many trees are used in the model\n",
    "\n",
    "- Hyperparameter Tuning: XGBoost has many hyperparameters that can be adjusted, making it important to properly tune the parameters to optimize performance. However, finding the optimal set of parameters can be time-consuming and requires expertise.\n",
    "\n",
    "- Memory Requirements: XGBoost can be memory-intensive, especially when working with large datasets, making it less suitable for systems with limited memory resources.\n",
    "\n",
    "## III) Python Implementation\n",
    "\n",
    "### 1) Hyperparameters tuning\n",
    "\n",
    "#### General Parameters\n",
    "\n",
    "The general parameter are here to access overall functionalities\n",
    "\n",
    "- booster Select the type of model to run at each iteration. It has 2 options:\n",
    "gbtree: tree-based models\n",
    "gblinear: linear models\n",
    "\n",
    "We will use in this project tree-based models due to its performance\n",
    "\n",
    "#### Booster Parameters for Tree Booster\n",
    "\n",
    "There are 2 types of booster parameters, one for linear and another for tree but we will only consider tree booster here.\n",
    "\n",
    "1. Eta is also known as the learning rate; changing this number makes the model more robust by decreasing the weights on each step.\n",
    "\n",
    "2. Max_depth, it is the same as what we saw for the random forest\n",
    "\n",
    "3. gamma \n",
    "Only when the resulting split results in a positive reduction in the loss function is a node split. The minimal loss reduction necessary to divide is specified by Gamma. This makes the algorithm more conservative. The values can and should change based on the loss function.\n",
    "\n",
    "4. subsample \n",
    "The percentage of observations that are random samples for each tree. Lower values make the algorithm more conservative and prevent overfitting, but too low values may result in underfitting.\n",
    "\n",
    "5. colsample_bytree, it is the same as max_features for random forest\n",
    "\n",
    "\n",
    "#### Learning Task Parameters\n",
    "\n",
    "1. objective \n",
    "This defines the loss function to be minimized. We will use \"multi: softmax\" which is a multiclass classification using the softmax objective. It will return predicted class.\n",
    "\n",
    "2. eval_metric \n",
    "The evaluation metrics are to be used for validation data.\n",
    "merror – Multiclass classification error rate \n",
    "mlogloss – Multiclass logloss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Aujoux\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 52637 records.\n",
      "The testing dataset has 13160 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code_produit</th>\n",
       "      <th>Secteur</th>\n",
       "      <th>Famille</th>\n",
       "      <th>abricot</th>\n",
       "      <th>abricots</th>\n",
       "      <th>acidifiant</th>\n",
       "      <th>acidifiant acidifiant</th>\n",
       "      <th>acidifiant antioxydant</th>\n",
       "      <th>acidifiant arome</th>\n",
       "      <th>acidifiant arome naturel</th>\n",
       "      <th>...</th>\n",
       "      <th>vitamine pp</th>\n",
       "      <th>vitamine vitamine</th>\n",
       "      <th>vitamine vitamine b</th>\n",
       "      <th>vitamine vitamine vitamine</th>\n",
       "      <th>vitamines</th>\n",
       "      <th>vitamines vitamine</th>\n",
       "      <th>vitamines vitamine vitamine</th>\n",
       "      <th>volaille</th>\n",
       "      <th>yaourt</th>\n",
       "      <th>yaourt brass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Yaourts et laits fermentes sucres classiques</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125377</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>453.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Yaourts et laits fermentes sucres classiques</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245026</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>455.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Yaourts et laits fermentes sucres classiques</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>456.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Yaourts et laits fermentes sucres classiques</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>460.0</td>\n",
       "      <td>Produits laitiers et desserts frais</td>\n",
       "      <td>Fromages frais nature non sucres gourmands</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65793</th>\n",
       "      <td>101536.0</td>\n",
       "      <td>Sirops et boissons concentrees a diluer</td>\n",
       "      <td>Sirops</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65794</th>\n",
       "      <td>101537.0</td>\n",
       "      <td>Sirops et boissons concentrees a diluer</td>\n",
       "      <td>Sirops</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65795</th>\n",
       "      <td>101540.0</td>\n",
       "      <td>Sirops et boissons concentrees a diluer</td>\n",
       "      <td>Sirops</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65796</th>\n",
       "      <td>101542.0</td>\n",
       "      <td>Sirops et boissons concentrees a diluer</td>\n",
       "      <td>Sirops</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65797</th>\n",
       "      <td>101543.0</td>\n",
       "      <td>Sirops et boissons concentrees a diluer</td>\n",
       "      <td>Sirops</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65797 rows × 1503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Code_produit                                  Secteur  \\\n",
       "0             450.0      Produits laitiers et desserts frais   \n",
       "1             453.0      Produits laitiers et desserts frais   \n",
       "2             455.0      Produits laitiers et desserts frais   \n",
       "3             456.0      Produits laitiers et desserts frais   \n",
       "4             460.0      Produits laitiers et desserts frais   \n",
       "...             ...                                      ...   \n",
       "65793      101536.0  Sirops et boissons concentrees a diluer   \n",
       "65794      101537.0  Sirops et boissons concentrees a diluer   \n",
       "65795      101540.0  Sirops et boissons concentrees a diluer   \n",
       "65796      101542.0  Sirops et boissons concentrees a diluer   \n",
       "65797      101543.0  Sirops et boissons concentrees a diluer   \n",
       "\n",
       "                                            Famille  abricot  abricots  \\\n",
       "0      Yaourts et laits fermentes sucres classiques      0.0       0.0   \n",
       "1      Yaourts et laits fermentes sucres classiques      0.0       0.0   \n",
       "2      Yaourts et laits fermentes sucres classiques      0.0       0.0   \n",
       "3      Yaourts et laits fermentes sucres classiques      0.0       0.0   \n",
       "4        Fromages frais nature non sucres gourmands      0.0       0.0   \n",
       "...                                             ...      ...       ...   \n",
       "65793                                        Sirops      0.0       0.0   \n",
       "65794                                        Sirops      0.0       0.0   \n",
       "65795                                        Sirops      0.0       0.0   \n",
       "65796                                        Sirops      0.0       0.0   \n",
       "65797                                        Sirops      0.0       0.0   \n",
       "\n",
       "       acidifiant  acidifiant acidifiant  acidifiant antioxydant  \\\n",
       "0        0.000000                    0.0                     0.0   \n",
       "1        0.000000                    0.0                     0.0   \n",
       "2        0.000000                    0.0                     0.0   \n",
       "3        0.000000                    0.0                     0.0   \n",
       "4        0.000000                    0.0                     0.0   \n",
       "...           ...                    ...                     ...   \n",
       "65793    0.137469                    0.0                     0.0   \n",
       "65794    0.058943                    0.0                     0.0   \n",
       "65795    0.110652                    0.0                     0.0   \n",
       "65796    0.119906                    0.0                     0.0   \n",
       "65797    0.000000                    0.0                     0.0   \n",
       "\n",
       "       acidifiant arome  acidifiant arome naturel  ...  vitamine pp  \\\n",
       "0                   0.0                       0.0  ...          0.0   \n",
       "1                   0.0                       0.0  ...          0.0   \n",
       "2                   0.0                       0.0  ...          0.0   \n",
       "3                   0.0                       0.0  ...          0.0   \n",
       "4                   0.0                       0.0  ...          0.0   \n",
       "...                 ...                       ...  ...          ...   \n",
       "65793               0.0                       0.0  ...          0.0   \n",
       "65794               0.0                       0.0  ...          0.0   \n",
       "65795               0.0                       0.0  ...          0.0   \n",
       "65796               0.0                       0.0  ...          0.0   \n",
       "65797               0.0                       0.0  ...          0.0   \n",
       "\n",
       "       vitamine vitamine  vitamine vitamine b  vitamine vitamine vitamine  \\\n",
       "0                    0.0                  0.0                         0.0   \n",
       "1                    0.0                  0.0                         0.0   \n",
       "2                    0.0                  0.0                         0.0   \n",
       "3                    0.0                  0.0                         0.0   \n",
       "4                    0.0                  0.0                         0.0   \n",
       "...                  ...                  ...                         ...   \n",
       "65793                0.0                  0.0                         0.0   \n",
       "65794                0.0                  0.0                         0.0   \n",
       "65795                0.0                  0.0                         0.0   \n",
       "65796                0.0                  0.0                         0.0   \n",
       "65797                0.0                  0.0                         0.0   \n",
       "\n",
       "       vitamines  vitamines vitamine  vitamines vitamine vitamine  volaille  \\\n",
       "0            0.0                 0.0                          0.0       0.0   \n",
       "1            0.0                 0.0                          0.0       0.0   \n",
       "2            0.0                 0.0                          0.0       0.0   \n",
       "3            0.0                 0.0                          0.0       0.0   \n",
       "4            0.0                 0.0                          0.0       0.0   \n",
       "...          ...                 ...                          ...       ...   \n",
       "65793        0.0                 0.0                          0.0       0.0   \n",
       "65794        0.0                 0.0                          0.0       0.0   \n",
       "65795        0.0                 0.0                          0.0       0.0   \n",
       "65796        0.0                 0.0                          0.0       0.0   \n",
       "65797        0.0                 0.0                          0.0       0.0   \n",
       "\n",
       "         yaourt  yaourt brass  \n",
       "0      0.125377           0.0  \n",
       "1      0.245026           0.0  \n",
       "2      0.241945           0.0  \n",
       "3      0.242882           0.0  \n",
       "4      0.000000           0.0  \n",
       "...         ...           ...  \n",
       "65793  0.000000           0.0  \n",
       "65794  0.000000           0.0  \n",
       "65795  0.000000           0.0  \n",
       "65796  0.000000           0.0  \n",
       "65797  0.000000           0.0  \n",
       "\n",
       "[65797 rows x 1503 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Thomas Aujoux\\Documents\\GitHub\\food-classification\\Data_Preprocessing\\data_clean\\clean.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.drop(labels=22426, axis=0)\n",
    "\n",
    "y = df[[\"Secteur\"]]\n",
    "df_features = df.drop([\"Code_produit\", \"Secteur\", \"Famille\"], axis=1)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "print(f'The training dataset has {len(X_train)} records.')\n",
    "print(f'The testing dataset has {len(X_test)} records.')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Study of the various hyperparameters\n",
    "\n",
    "Next, we'll look at each of the hyperparameters to understand the consequences of changing them on the model.\n",
    "\n",
    "#### Maximum Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937082066869301 0.9443062295916467\n",
      "6\n",
      "0.9532674772036475 0.9557990170179018\n",
      "10\n",
      "0.9569908814589666 0.9588462181510232\n",
      "15\n",
      "0.9583586626139817 0.9596482247974817\n",
      "20\n",
      "0.9567629179331307 0.9576973363042032\n",
      "50\n",
      "0.9579787234042553 0.959043603006687\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "opt_table_estimators_accuracy=list()\n",
    "opt_table_estimators_f1=list()\n",
    "list_para = [3, 6, 10, 15, 20, 50, 100, 200, 300, 500, 900]\n",
    "\n",
    "for i in list_para:\n",
    "    print(i)\n",
    "    model = XGBClassifier(max_depth = i, objective='multi:softmax',num_class=31)\n",
    "    model.fit(X_train,y_train)\n",
    "    output=model.predict(X_test)\n",
    "    opt_table_estimators_accuracy.append(accuracy_score(y_test, output))\n",
    "    opt_table_estimators_f1.append(f1_score(y_test, output, average='macro'))\n",
    "    print(accuracy_score(y_test, output),f1_score(y_test, output, average='macro'))\n",
    "plt.plot(list_para, opt_table_estimators_accuracy)\n",
    "plt.plot(list_para, opt_table_estimators_f1)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Random Forest Score')\n",
    "plt.title('Random Forest Score VS Number of trees (5 features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze the maximum depth as follows: before the maximum depth is 10, our model underfits, and after 10, it overfits. This analysis has enabled us to find a good estimate of this parameter. The number we can estimate as satisfactory is 10.\n",
    "\n",
    "#### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[12:01:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:802: Exception in gpu_hist: [12:01:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\data\\../common/device_helpers.cuh:431: Memory allocation error on worker 0: bad allocation: cudaErrorMemoryAllocation: out of memory\n- Free memory: 214305178\n- Requested memory: 1073741824\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m      7\u001b[0m model \u001b[39m=\u001b[39m XGBClassifier(learning_rate \u001b[39m=\u001b[39m i, tree_method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu_hist\u001b[39m\u001b[39m\"\u001b[39m, objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmulti:softmax\u001b[39m\u001b[39m'\u001b[39m,num_class\u001b[39m=\u001b[39m\u001b[39m31\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[0;32m      9\u001b[0m output\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     10\u001b[0m opt_table_estimators_accuracy\u001b[39m.\u001b[39mappend(accuracy_score(y_test, output))\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [12:01:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:802: Exception in gpu_hist: [12:01:31] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\data\\../common/device_helpers.cuh:431: Memory allocation error on worker 0: bad allocation: cudaErrorMemoryAllocation: out of memory\n- Free memory: 214305178\n- Requested memory: 1073741824\n\n"
     ]
    }
   ],
   "source": [
    "opt_table_estimators_accuracy=list()\n",
    "opt_table_estimators_f1=list()\n",
    "list_para = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8]\n",
    "\n",
    "for i in list_para:\n",
    "    print(i)\n",
    "    model = XGBClassifier(learning_rate = i, tree_method=\"gpu_hist\", objective='multi:softmax',num_class=31)\n",
    "    model.fit(X_train,y_train)\n",
    "    output=model.predict(X_test)\n",
    "    opt_table_estimators_accuracy.append(accuracy_score(y_test, output))\n",
    "    opt_table_estimators_f1.append(f1_score(y_test, output, average='macro'))\n",
    "    print(accuracy_score(y_test, output),f1_score(y_test, output, average='macro'))\n",
    "plt.plot(list_para, opt_table_estimators_accuracy)\n",
    "plt.plot(list_para, opt_table_estimators_f1)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Random Forest Score')\n",
    "plt.title('Random Forest Score VS Number of trees (5 features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say the same thing, the optimal number for the learning rate is 0.6.\n",
    "\n",
    "#### Gama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m list_para:\n\u001b[0;32m      6\u001b[0m     model \u001b[39m=\u001b[39m XGBClassifier(gamma \u001b[39m=\u001b[39m i, objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmulti:softmax\u001b[39m\u001b[39m'\u001b[39m,num_class\u001b[39m=\u001b[39m\u001b[39m31\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[0;32m      8\u001b[0m     output\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m      9\u001b[0m     opt_table_estimators_accuracy\u001b[39m.\u001b[39mappend(accuracy_score(y_test, output))\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas Aujoux\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt_table_estimators_accuracy=list()\n",
    "opt_table_estimators_f1=list()\n",
    "list_para = [0, 0.01, 0.1, 1, 10]\n",
    "\n",
    "for i in list_para:\n",
    "    model = XGBClassifier(gamma = i, objective='multi:softmax',num_class=31)\n",
    "    model.fit(X_train,y_train)\n",
    "    output=model.predict(X_test)\n",
    "    opt_table_estimators_accuracy.append(accuracy_score(y_test, output))\n",
    "    opt_table_estimators_f1.append(f1_score(y_test, output, average='macro'))\n",
    "plt.plot(list_para, opt_table_estimators_accuracy)\n",
    "plt.plot(list_para, opt_table_estimators_f1)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Random Forest Score')\n",
    "plt.title('Random Forest Score VS Number of trees (5 features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For the Gamma parameter it is 1\n",
    "\n",
    "#### Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_table_estimators_accuracy=list()\n",
    "opt_table_estimators_f1=list()\n",
    "list_para = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "for i in list_para:\n",
    "    model = XGBClassifier(subsample = i, objective='multi:softmax',num_class=7)\n",
    "    model.fit(X_train,y_train)\n",
    "    output=model.predict(X_test)\n",
    "    opt_table_estimators_accuracy.append(accuracy_score(y_test, output))\n",
    "    opt_table_estimators_f1.append(f1_score(y_test, output, average='macro'))\n",
    "plt.plot(list_para, opt_table_estimators_accuracy)\n",
    "plt.plot(list_para, opt_table_estimators_f1)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Random Forest Score')\n",
    "plt.title('Random Forest Score VS Number of trees (5 features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal parameter for the subsample is 0.8\n",
    "\n",
    "#### Colsample by Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_table_estimators_accuracy=list()\n",
    "opt_table_estimators_f1=list()\n",
    "list_para = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "for i in list_para:\n",
    "    model = XGBClassifier(colsample_bytree = i, objective='multi:softmax',num_class=7)\n",
    "    model.fit(X_train,y_train)\n",
    "    output=model.predict(X_test)\n",
    "    opt_table_estimators_accuracy.append(accuracy_score(y_test, output))\n",
    "    opt_table_estimators_f1.append(f1_score(y_test, output, average='macro'))\n",
    "plt.plot(list_para, opt_table_estimators_accuracy)\n",
    "plt.plot(list_para, opt_table_estimators_f1)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Random Forest Score')\n",
    "plt.title('Random Forest Score VS Number of trees (5 features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparagent for the parameter colsample by tree the model doesn't manage to overfitter, in which case we can't yet find an ideal parameter. So we're going to introduce Grid Search.\n",
    "### 3) Grid Search automatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': [10],\n",
    "            'learning_rate': [0.3],\n",
    "            'gamma': [0.001],\n",
    "            'subsample': [1],\n",
    "            'colsample_bytree': [1],\n",
    "            'eval_metric': ['mlogloss']\n",
    "            }\n",
    "\n",
    "model = XGBClassifier\n",
    "\n",
    "\n",
    "def model_best_param(X_train, X_test, y_train, y_test, model, params):\n",
    "\n",
    "    pipe_nb = make_pipeline(\n",
    "    model(num_class=7, objective='multi:softmax')\n",
    "    )\n",
    "    grid_search = GridSearchCV(estimator=model(), param_grid=params, verbose=2, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    output = grid_search.predict(X_test)\n",
    "    \n",
    "    output = le.inverse_transform(output)\n",
    "    y_test = le.inverse_transform(y_test)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test, output)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"DJ\", \"chill\", \"date\", \"melancholy\", \"party\", \"sport\", \"study\"])\n",
    "    cm_display.plot()\n",
    "    plt.show() \n",
    "\n",
    "    print(classification_report(y_test, output))\n",
    "\n",
    "    return(grid_search.best_params_)\n",
    "\n",
    "model_best_param(X_train, X_test, y_train, y_test, model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Analysis of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "    objective='multi: softprob',\n",
    "    max_depth = 10,\n",
    "    learning_rate = 0.3,\n",
    "    gamma = 0.001,\n",
    "    subsample = 1,\n",
    "    colsample_bytree = 1,\n",
    "    eval_metric = 'mlogloss'\n",
    ")\n",
    "\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "plot_importance(model, ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
