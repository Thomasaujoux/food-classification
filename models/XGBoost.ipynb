{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Multi-class Classification\n",
    "\n",
    "XGBoost is a gradient boosting ensemble Machine Learning technique based on decision trees.\n",
    "We've already seen what the terms decision tree, bagging and random forest mean in the Random Forest section, so let's take a closer look at what's new in XGBoost.\n",
    "\n",
    "## What does boosting mean ?\n",
    "\n",
    "Boosting is an ensemble strategy that adds new models to repair faults generated by current models. First, a model is constructed using the training data. The second model is then constructed in an attempt to address the faults in the previous model and models are added sequentially until no further advancements are possible. This ensemble method attempts to build a powerful classifier using previously 'weaker' classifiers.\n",
    "\n",
    "## What does gradient means ?\n",
    "\n",
    "Gradient boosting is so named because it employs a gradient descent approach to minimize loss when adding new models.\n",
    "\n",
    "## What does eXtreme mean\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an advanced implementation of a gradient boosting algorithm.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "-  XGBoost Execution Speed:\n",
    "XGBoost is generally quick compared to other gradient boosting implementations.\n",
    "\n",
    "- Performance: XGBoost has a strong track record of producing high-quality results in various machine learning tasks, especially in Kaggle competitions, where it has been a popular choice for winning solutions.\n",
    "\n",
    "- Regularization :\n",
    "Standard GBM implementation has no regularization like XGBoost; therefore, it also helps to reduce overfitting. In fact, XGBoost is also known as a ‘regularized boosting‘ technique.\n",
    "\n",
    "- High Flexibility:\n",
    "XGBoost allows users to define custom optimization objectives and evaluation criteria. This adds a whole new dimension to the model and there is no limit to what we can do.\n",
    "\n",
    "- Handling Missing Values:\n",
    "XGBoost has an in-built routine to handle missing values. The user is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in the future.\n",
    "\n",
    "- Interpretability: Unlike some machine learning algorithms that can be difficult to interpret, XGBoost provides feature importances, allowing for a better understanding of which variables are most important in making predictions.\n",
    "\n",
    "## Drawbacks\n",
    "\n",
    "- Computational Complexity:\n",
    "XGBoost can be computationally intensive, especially when training large models, making it less suitable for resource-constrained systems.\n",
    "\n",
    "- Overfitting: XGBoost can be prone to overfitting, especially when trained on small datasets or when too many trees are used in the model\n",
    "\n",
    "- Hyperparameter Tuning: XGBoost has many hyperparameters that can be adjusted, making it important to properly tune the parameters to optimize performance. However, finding the optimal set of parameters can be time-consuming and requires expertise.\n",
    "\n",
    "- Memory Requirements: XGBoost can be memory-intensive, especially when working with large datasets, making it less suitable for systems with limited memory resources.\n",
    "\n",
    "## Tune Hyperparameters\n",
    "\n",
    "### General Parameters\n",
    "\n",
    "The general parameter are here to access overall functionalities\n",
    "\n",
    "- booster Select the type of model to run at each iteration. It has 2 options:\n",
    "gbtree: tree-based models\n",
    "gblinear: linear models\n",
    "\n",
    "We will use in this project tree-based models due to its performance\n",
    "\n",
    "### Booster Parameters for Tree Booster\n",
    "\n",
    "eta [default=0.3]\n",
    "\n",
    "    Analogous to the learning rate in GBM\n",
    "    Makes the model more robust by shrinking the weights on each step\n",
    "    Typical final values to be used: 0.01-0.2\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "### Learning Task Parameters\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
